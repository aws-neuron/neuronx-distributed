{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3.2-1b Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [meta-llama/Llama-3.2-1b](https://huggingface.co/meta-llama/Llama-3.2-1B) model for tensor parallel inference on Neuron using the `Neuronx-Distributed` package.\n",
    "\n",
    "> Note: This model is not currently optimized for performance on neuronx-distributed. It serves as a tutorial for hosting a 1B Llama model on AWS Tranium and integrating it with kernels from [NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html). \n",
    "\n",
    "The example has the following main sections:\n",
    "\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download the model\n",
    "1. Trace the model\n",
    "1. Perform greedy sampling\n",
    "1. Benchmark sampling\n",
    "\n",
    "This Jupyter Notebook can be run on a Trn1 instance (`trn1.2xlarge`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook\n",
    "\n",
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [Neuronx-Distributed](https://github.com/aws-neuron/neuronx-distributed.git) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/neuronx-distributed.git\n",
    "```\n",
    "\n",
    "2. Navigate to the `examples/inference` samples folder\n",
    "```\n",
    "cd neuronx-distributed/example/inference/\n",
    "```\n",
    "\n",
    "3. Follow the instructions on `examples/inference/README.md` to set up the virtual environment.\n",
    "\n",
    "4. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance, or run using VSCode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `neuronx-distributed`\n",
    "\n",
    "You can install `neuronx-distributed` using the [setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/index.html). Most of other packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model \n",
    "Use of this model is governed by the Meta license. In order to download the model weights and tokenizer follow the instructions in [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B). \n",
    "\n",
    "Once granted access, you can download the model. For the purposes of this sample we assume you have saved the Llama-3.2-1b model in a directory called `models/llama-3.2-1b` with the following format:\n",
    "\n",
    "```\n",
    "llama-3.2-1b\n",
    " ├── original\n",
    "     ├── consolidated.00.pth\n",
    "     ├── params.json\n",
    "     ├── tokenizer.model\n",
    " ├── config.json\n",
    " ├── generation_config.json\n",
    " ├── LICENSE.txt\n",
    " ├── model.safetensors\n",
    " ├── README.md\n",
    " ├── special_tokens_map.json\n",
    " ├── tokenizer_config.json\n",
    " ├── tokenizer.json\n",
    " ├── USE_POLICY.md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/models/llama-3.2-1b\"\n",
    "traced_model_path = \"/home/ubuntu/models/llama-3.2-1b-trace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace and load the model\n",
    "\n",
    "Now we can trace the model using the LlamaRunner script. This saves the model to the `traced_model_path`. After tracing, the model can be loaded.\n",
    "\n",
    "In this sample we use tensor parallelism degree 2 to optimize performance on trn1.2xlarge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama3.llama3_runner import LlamaRunner\n",
    "\n",
    "max_context_length = 2048\n",
    "max_new_tokens = 256\n",
    "batch_size = 1\n",
    "tp_degree = 2\n",
    "\n",
    "runner = LlamaRunner(model_path=model_path, \n",
    "                     tokenizer_path=model_path)\n",
    "\n",
    "runner.trace(traced_model_path=traced_model_path,\n",
    "             tp_degree=tp_degree,\n",
    "             batch_size=batch_size,\n",
    "             context_lengths=max_context_length,\n",
    "             new_token_counts=max_new_tokens,\n",
    "             on_device_sampling=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "Now lets use the model to perform autoregressive sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_model = runner.load_neuron_model(traced_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"I believe the meaning of life is\"]\n",
    "\n",
    "generate_ids, outputs = runner.generate_on_neuron(prompt, neuron_model)\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    print(f\"output {idx}: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking \n",
    "\n",
    "Here we benchmark the per token latency for greedy sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = runner.benchmark_sampling(neuron_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
